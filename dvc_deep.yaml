stages:
  prepare_data:
    cmd: python experiment/prepare_data.py
    outs:
      - assets/data/df_raw.csv
      - assets/data/splits/train/raw.csv
      - assets/data/splits/val/raw.csv
      - assets/data/splits/test/raw.csv

  text_normalization: # text_normalization
    cmd: python experiment/preprocessing.py
    deps:
      - assets/data/splits/train/raw.csv
      - assets/data/splits/val/raw.csv
      - assets/data/splits/test/raw.csv
    outs:
      - assets/data/splits/train/preprocessed.csv
      - assets/data/splits/val/preprocessed.csv
      - assets/data/splits/test/preprocessed.csv

  feat_extract_deep:
    cmd: python experiment/supervised_deep_models/feat_extract_deep.py
    deps:
      - assets/data/splits/train/preprocessed.csv
      - assets/data/splits/val/preprocessed.csv
    outs:
      - assets/deep_assets/word2vec.model
      - assets/data/splits/train/padded.csv
      - assets/data/splits/val/padded.csv

  tuning_models:
    cmd: python experiment/supervised_deep_models/tuning_models.py
    deps:
      - assets/data/splits/train/padded.csv
      - assets/data/splits/val/padded.csv
      - assets/deep_assets/word2vec.model
      - assets/deep_assets/tokenizer.json
    outs:
      - assets/deep_assets/lstm_model
      - assets/deep_assets/bilstm_model

  evaluate_models:
    cmd: python experiment/supervised_deep_models/evaluate_models.py
    deps:
      - assets/data/splits/test/preprocessed.csv
      - assets/deep_assets/word2vec.model
      - assets/deep_assets/tokenizer.json
      - assets/deep_assets/lstm_model
      - assets/deep_assets/bilstm_model
    outs:
      - assets/deep_assets/deep_results.csv # or graph similar to the one in the paper

#  apply_model:
#    cmd: python experiment/supervised_deep_models/apply_model.py
#    deps:
#      - assets/data/splits/test/raw.csv
#      - assets/deep_assets/word2vec.model
#      - assets/deep_assets/tokenizer.json
#      - assets/deep_assets/lstm_model
#    outs:
#      - assets/deep_assets/predictions.csv