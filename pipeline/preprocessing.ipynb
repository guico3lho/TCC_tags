{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Packages"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def transformDocument(df, column_name, language):\n",
    "    stop_words = usingStopwords(language)\n",
    "    # 1. Aplicar preprocessamento nos títulos e textos completos\n",
    "    if language == 'pt':\n",
    "        # Substituir símbolos importantes\n",
    "        df[column_name] = df[column_name].map(lambda s: s.replace('-feira', ''))\n",
    "        df[column_name] = df[column_name].map(lambda s: s.replace('+', 'mais '))\n",
    "        df[column_name] = df[column_name].map(lambda s: s.replace('-', 'menos '))\n",
    "        df[column_name] = df[column_name].map(lambda s: s.replace('%', ' por cento'))\n",
    "        df[column_name] = df[column_name].map(lambda s: removeStopwords(s, stop_words))\n",
    "\n",
    "    elif language == 'en':\n",
    "        df[column_name] = df[column_name].map(lambda s: s.replace('-', 'less'))\n",
    "        df[column_name] = df[column_name].map(lambda s: s.replace('+', 'plus '))\n",
    "        df[column_name] = df[column_name].map(lambda s: s.replace('%', ' percent'))\n",
    "        df[column_name] = df[column_name].map(lambda s: removeStopwords(s, stop_words))\n",
    "\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    df[column_name] = df[column_name].map(lambda s: s.replace('R$', ''))\n",
    "    df[column_name] = df[column_name].map(lambda s: s.replace('U$', ''))\n",
    "    df[column_name] = df[column_name].map(lambda s: s.replace('US$', ''))\n",
    "    df[column_name] = df[column_name].map(lambda s: s.replace('S&P 500', 'spx'))\n",
    "\n",
    "    # Transformar em String e Letras Minúsculas nas Mensagens\n",
    "    df[column_name] = df[column_name].map(lambda s:\n",
    "                                              normalizarString(s))\n",
    "\n",
    "    # Remover Pontuações\n",
    "    # Remover Pontuações\n",
    "    df[column_name] = df[column_name].map(lambda s: s.translate(str.maketrans('', '', string.punctuation)))\n",
    "\n",
    "    # Remover Emojis\n",
    "    df[column_name] = df[column_name].map(lambda s: removeEmojis(s))\n",
    "\n",
    "    # Quebras de Linha desnecessárias\n",
    "    df[column_name] = df[column_name].map(lambda s: s.replace('\\n', ' '))\n",
    "\n",
    "    # Remover aspas duplas\n",
    "    df[column_name] = df[column_name].map(lambda s: s.replace('\\\"', ''))\n",
    "    df[column_name] = df[column_name].map(lambda s: s.replace('“', ''))\n",
    "    df[column_name] = df[column_name].map(lambda s: s.replace('”', ''))\n",
    "\n",
    "    # Remover valores\n",
    "    df[column_name] = df[column_name].map(lambda s: removeValores(s))\n",
    "\n",
    "    # Espaços desnecessários\n",
    "    df[column_name] = df[column_name].map(lambda s: s.strip())\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def removeEmojis(sentence):\n",
    "    \"Remoção de Emojis nas mensagens de texto.\"\n",
    "\n",
    "    # Padrões dos Emojis\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u'\\U00010000-\\U0010ffff'\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\u3030\"\n",
    "                               u\"\\ufe0f\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "\n",
    "    return emoji_pattern.sub(r'', sentence)\n",
    "\n",
    "def removeValores(sentence):\n",
    "    new_sentece = ''\n",
    "\n",
    "    for token in sentence.split():\n",
    "        if token.isdigit():\n",
    "            token = '<NUM>'\n",
    "        new_sentece += ' {}'.format(token)\n",
    "\n",
    "    return new_sentece\n",
    "\n",
    "def usingStopwords(language):\n",
    "    stop_words = []\n",
    "\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "    if language == 'pt':\n",
    "        stop_words = nltk.corpus.stopwords.words('portuguese')\n",
    "    elif language == 'en':\n",
    "        stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "    return stop_words\n",
    "\n",
    "def removeStopwords(text, stop_words):\n",
    "    tokens = []\n",
    "    for word in text.split():\n",
    "        if word not in stop_words:\n",
    "            tokens.append(word)\n",
    "\n",
    "    text = ' '.join(tokens)\n",
    "    return text\n",
    "\n",
    "def normalizarString(text):\n",
    "    \"\"\"\n",
    "    Função para retirar acentuações e converter para minúscula\n",
    "    :param text:\n",
    "    :return text_normalizado\n",
    "    \"\"\"\n",
    "    import unicodedata\n",
    "\n",
    "    text = unicodedata.normalize('NFD', text).encode('ascii', 'ignore').decode(\"utf-8\")\n",
    "    return str(text.lower())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "train_raw = pd.read_csv('../assets/data/train/raw.csv')\n",
    "val_raw = pd.read_csv('../assets/data/val/raw.csv')\n",
    "test_raw = pd.read_csv('../assets/data/test/raw.csv')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocessing dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\guimi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\guimi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\guimi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_pp = transformDocument(train_raw, 'title', 'pt')\n",
    "val_pp = transformDocument(val_raw, 'title', 'pt')\n",
    "test_pp = transformDocument(test_raw, 'title', 'pt')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exporting train, val and test for word2vec and EDA"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "train_pp.to_csv('../assets/data/train/preprocessed.csv', index=False)\n",
    "val_pp.to_csv('../assets/data/val/preprocessed.csv', index=False)\n",
    "test_pp.to_csv('../assets/data/test/preprocessed.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
