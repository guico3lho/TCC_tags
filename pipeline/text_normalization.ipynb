{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Packages"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T01:31:39.181440200Z",
     "start_time": "2023-12-01T01:31:33.994055400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Main funcs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def normalizar_texto_coluna_df(df: pd.DataFrame, column_name: str, language: str):\n",
    "\n",
    "    df_copy = df.copy()\n",
    "    df_transformado = transformar_texto_coluna_df(df_copy, column_name, language)\n",
    "    df_tokenized = tokenizar_texto_coluna_df(df_transformado, column_name)\n",
    "    df_without_stopwords = remover_stopwords_coluna_df(df_tokenized, column_name, language)\n",
    "    df_stemmed = stemizar_tokens_coluna_df(df_without_stopwords, column_name)\n",
    "    df_resultado_final = get_resultado_final(df_stemmed, column_name)\n",
    "\n",
    "    df_normalizado = df_resultado_final.copy()\n",
    "    return df_normalizado\n",
    "    # df_without_stopwords = remover_stopwords_coluna_df(df_transformado, column_name, language)\n",
    "\n",
    "\n",
    "def transformar_texto_coluna_df(df: pd.DataFrame, column_name: str, language: str):\n",
    "    # transformação de texto (default case, emoji, symbols, regular expressions)\n",
    "    # transformar_texto\n",
    "    df_transformado = df.copy()\n",
    "\n",
    "    df_transformado[column_name] = df_transformado[column_name].map(lambda s:\n",
    "                                                                    remove_acentos(s))\n",
    "    \n",
    "    df_transformado[column_name] = df_transformado[column_name].map(lambda s: s.lower())\n",
    "    # 1. Aplicar preprocessamento nos títulos e textos completos\n",
    "    if language == 'pt':\n",
    "        # Substituir símbolos importantes\n",
    "        df_transformado[column_name] = df_transformado[column_name].map(lambda s: s.replace('-feira', ''))\n",
    "        df_transformado[column_name] = df_transformado[column_name].map(lambda s: s.replace('+', 'mais '))\n",
    "        df_transformado[column_name] = df_transformado[column_name].map(lambda s: s.replace('-', 'menos '))\n",
    "        df_transformado[column_name] = df_transformado[column_name].map(lambda s: s.replace('%', ' por cento'))\n",
    "\n",
    "    elif language == 'en':\n",
    "        df_transformado[column_name] = df_transformado[column_name].map(lambda s: s.replace('-', 'less'))\n",
    "        df_transformado[column_name] = df_transformado[column_name].map(lambda s: s.replace('+', 'plus '))\n",
    "        df_transformado[column_name] = df_transformado[column_name].map(lambda s: s.replace('%', ' percent'))\n",
    "\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    df_transformado[column_name] = df_transformado[column_name].map(lambda s: s.replace('r$', ''))\n",
    "    df_transformado[column_name] = df_transformado[column_name].map(lambda s: s.replace('u$', ''))\n",
    "    df_transformado[column_name] = df_transformado[column_name].map(lambda s: s.replace('us$', ''))\n",
    "    df_transformado[column_name] = df_transformado[column_name].map(lambda s: s.replace('s&p 500', 'spx'))\n",
    "\n",
    "    # Transformar em String e Letras Minúsculas nas Mensagens\n",
    "\n",
    "    # Remover Pontuações\n",
    "    df_transformado[column_name] = df_transformado[column_name].map(\n",
    "        lambda s: s.translate(str.maketrans('', '', string.punctuation)))\n",
    "\n",
    "    # Remover Emojis\n",
    "    df_transformado[column_name] = df_transformado[column_name].map(lambda s: remove_emojis(s))\n",
    "\n",
    "    # Quebras de Linha desnecessárias\n",
    "    df_transformado[column_name] = df_transformado[column_name].map(lambda s: s.replace('\\n', ' '))\n",
    "\n",
    "    # Remover aspas duplas\n",
    "    df_transformado[column_name] = df_transformado[column_name].map(lambda s: s.replace('\\\"', ''))\n",
    "    df_transformado[column_name] = df_transformado[column_name].map(lambda s: s.replace('“', ''))\n",
    "    df_transformado[column_name] = df_transformado[column_name].map(lambda s: s.replace('”', ''))\n",
    "\n",
    "    # Remover valores\n",
    "    df_transformado[column_name] = df_transformado[column_name].map(lambda s: remove_numeros(s))\n",
    "\n",
    "    # Espaços desnecessários\n",
    "    df_transformado[column_name] = df_transformado[column_name].map(lambda s: s.strip())\n",
    "    return df_transformado\n",
    "\n",
    "def tokenizar_texto_coluna_df(df: pd.DataFrame, column_name: str):\n",
    "    df_tokenized = df.copy()\n",
    "    from nltk.tokenize import RegexpTokenizer\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    df_tokenized[column_name] = df_tokenized[column_name].map(lambda s: tokenizer.tokenize(s))\n",
    "    return df_tokenized\n",
    "\n",
    "def remover_stopwords_coluna_df(df: pd.DataFrame, column_name: str, language: str):\n",
    "    df_without_stopwords = df.copy()\n",
    "    stop_words = get_stopwords(language)\n",
    "    df_without_stopwords[column_name] = df_without_stopwords[column_name].map(\n",
    "        lambda tokens: remove_stopwords(tokens, stop_words))\n",
    "    return df_without_stopwords\n",
    "\n",
    "def stemizar_tokens_coluna_df(df: pd.DataFrame, column_name: str):\n",
    "    \n",
    "    df_stemized = df.copy()\n",
    "    from nltk.stem import SnowballStemmer\n",
    "    stemmer = SnowballStemmer('portuguese')\n",
    "    df_stemized[column_name] = df_stemized[column_name].map(lambda tokens: [stemmer.stem(token) for token in tokens])\n",
    "    return df_stemized\n",
    "\n",
    "def get_resultado_final(df: pd.DataFrame, column_name: str):\n",
    "    df_resultado_final = df.copy()\n",
    "    df_resultado_final[column_name] = df_resultado_final[column_name].map(lambda s: ' '.join(s))\n",
    "    return df_resultado_final\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T01:31:39.205103700Z",
     "start_time": "2023-12-01T01:31:39.183441800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Aux funcs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def remove_emojis(sentence):\n",
    "    \"Remoção de Emojis nas mensagens de texto.\"\n",
    "\n",
    "    # Padrões dos Emojis\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u'\\U00010000-\\U0010ffff'\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\u3030\"\n",
    "                               u\"\\ufe0f\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "\n",
    "    return emoji_pattern.sub(r'', sentence)\n",
    "\n",
    "\n",
    "def remove_numeros(sentence):\n",
    "    new_sentece = ''\n",
    "\n",
    "    for token in sentence.split():\n",
    "        if token.isdigit():\n",
    "            token = '<NUM>'\n",
    "        new_sentece += ' {}'.format(token)\n",
    "\n",
    "    return new_sentece\n",
    "\n",
    "\n",
    "def get_stopwords(language):\n",
    "    stop_words = []\n",
    "\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "    if language == 'pt':\n",
    "        stop_words = nltk.corpus.stopwords.words('portuguese')\n",
    "    elif language == 'en':\n",
    "        stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "    return stop_words\n",
    "\n",
    "\n",
    "def remove_stopwords(tokens, stop_words):\n",
    "    tokens_without_sw = []\n",
    "    for word in tokens:\n",
    "        if word not in stop_words:\n",
    "            tokens_without_sw.append(word)\n",
    "\n",
    "    return tokens_without_sw\n",
    "\n",
    "\n",
    "def remove_acentos(text):\n",
    "    \"\"\"\n",
    "    Função para retirar acentuações e converter para minúscula\n",
    "    :param text:\n",
    "    :return text_normalizado\n",
    "    \"\"\"\n",
    "    import unicodedata\n",
    "\n",
    "    text = unicodedata.normalize('NFD', text).encode('ascii', 'ignore').decode(\"utf-8\")\n",
    "    return text"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T01:31:39.230592100Z",
     "start_time": "2023-12-01T01:31:39.205103700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Scratches"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# df_vale3_without_valestr = pd.read_csv('../assets/data/vale3_without_valestr.csv', index_col=0)\n",
    "# df_normalizado = normalizar_texto_coluna_df(df_vale3_without_valestr, 'title', 'pt')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T01:31:39.230592100Z",
     "start_time": "2023-12-01T01:31:39.219896600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dependencies"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "                                               title  label\n0  Engie Brasil define data do pagamento de R$ 89...      3\n1  Klabin anuncia parceria para a exploração de f...      0\n2  Pré-Sal Petróleo comercializa gás da União do ...      1\n3  Petrobras (PETR4) contrata linha de crédito de...      1\n4  Avianca Brasil pode perder 20% da frota de aviões      0\n5  Ibovespa abre em queda de 1,5% com mau humor e...      2\n6  Ibovespa abre em alta com PIB da China e refor...      0\n7  Agenda do Dia: Oi; Rumo; Sabesp; brMalls; B2W;...      0\n8  Vale (VALE3) anuncia suspensão temporária de o...      2\n9  Analistas veem venda de ativos da Vale para Mo...      2",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Engie Brasil define data do pagamento de R$ 89...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Klabin anuncia parceria para a exploração de f...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Pré-Sal Petróleo comercializa gás da União do ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Petrobras (PETR4) contrata linha de crédito de...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Avianca Brasil pode perder 20% da frota de aviões</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Ibovespa abre em queda de 1,5% com mau humor e...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Ibovespa abre em alta com PIB da China e refor...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Agenda do Dia: Oi; Rumo; Sabesp; brMalls; B2W;...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Vale (VALE3) anuncia suspensão temporária de o...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Analistas veem venda de ativos da Vale para Mo...</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_raw = pd.read_csv('../assets/data/splits/train/raw.csv')\n",
    "val_raw = pd.read_csv('../assets/data/splits/val/raw.csv')\n",
    "test_raw = pd.read_csv('../assets/data/splits/test/raw.csv')\n",
    "df_raw = pd.concat([train_raw, val_raw, test_raw])\n",
    "df_raw.sample(10, random_state=42)[['title', 'label']].reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T01:31:39.477954600Z",
     "start_time": "2023-12-01T01:31:39.227182100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocessing splits"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Guilherme\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Guilherme\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Guilherme\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                               title  label\n0  engi brasil defin dat pagament num milho dividend      3\n1          klabin anunc parc exploraca florest paran      0\n2  premen sal petrol comercializ gas unia camp su...      1\n3  petrobr petr4 contrat linh credit num bilho nu...      1\n4          avianc brasil pod perd num cent frot avio      0\n5  ibovesp abre qued num cent mau humor extern ba...      2\n6           ibovesp abre alta pib chin reform tribut      0\n7       agend dia oi rum sabesp brmalls b2w suzan b3      0\n8  val vale3 anunc suspensa tempor operaco estrad...      2\n9               anal veem vend ativ val mosaic posit      2",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>engi brasil defin dat pagament num milho dividend</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>klabin anunc parc exploraca florest paran</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>premen sal petrol comercializ gas unia camp su...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>petrobr petr4 contrat linh credit num bilho nu...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>avianc brasil pod perd num cent frot avio</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>ibovesp abre qued num cent mau humor extern ba...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>ibovesp abre alta pib chin reform tribut</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>agend dia oi rum sabesp brmalls b2w suzan b3</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>val vale3 anunc suspensa tempor operaco estrad...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>anal veem vend ativ val mosaic posit</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pp = normalizar_texto_coluna_df(train_raw, 'title', 'pt')\n",
    "val_pp = normalizar_texto_coluna_df(val_raw, 'title', 'pt')\n",
    "test_pp = normalizar_texto_coluna_df(test_raw, 'title', 'pt')\n",
    "df_pp = pd.concat([train_pp, val_pp, test_pp])\n",
    "df_pp.sample(10, random_state=42)[['title', 'label']].reset_index(drop=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T01:31:45.215339500Z",
     "start_time": "2023-12-01T01:31:39.474955Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Outputs for EDA, word2vec and preprocessing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "train_pp.to_csv('../assets/data/splits/train/normalized.csv', index=False)\n",
    "val_pp.to_csv('../assets/data/splits/val/normalized.csv', index=False)\n",
    "test_pp.to_csv('../assets/data/splits/test/normalized.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T01:31:45.396449900Z",
     "start_time": "2023-12-01T01:31:45.210194100Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
